










!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!







                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      

                      
                      
                      
                      
                      
                      

                      

                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                     
                      
!    RVTK test (Restartability or Parallel reproducibility)








                      
                      
                      
                      
                      


!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!






















































































































!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!





















!









!-# define float dfloat
!-# define FLoaT dfloat
!-# define FLOAT dfloat
!-# define sqrt dsqrt
!-# define SQRT dsqrt
!-# define exp dexp
!-# define EXP dexp
!-# define dtanh dtanh
!-# define TANH dtanh









      subroutine MessPass3D_tile (Istr,Iend,Jstr,Jend, A, nmax)
!
!======================================================================
!
! This subroutine is designed for CROCO- code. It exchanges domain
! boundary information, including 2 (or 3) ghost-cells in each
! direction.
!
! Ping Wang 9/15/99.
! Patrick Marchesiello 2012: generic number of ghost-cells Npts
!
!======================================================================
!
!     implicit none
!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!
!----------------------------------------------------------------------
! Dimensions of Physical Grid and array dimensions
!----------------------------------------------------------------------
!
! LLm,MMm  Number of the internal points of the PHYSICAL grid.
!          in the XI- and ETA-directions [physical side boundary
!          points and peroodic ghost points (if any) are excluded].
!
! Lm,Mm    Number of the internal points [see above] of array
!          covering a Message Passing subdomain. In the case when
!          no Message Passing partitioning is used, these two are
!          the same as LLm,MMm.
!
! N        Number of vertical levels.
!
      integer  LLm,Lm,MMm,Mm,N, LLm0,MMm0
      parameter (LLm0=41,   MMm0=42,   N=32)   ! 

      parameter (LLm=LLm0,  MMm=MMm0)

!
!----------------------------------------------------------------------
! Number of layers in Sediment (SL)
!----------------------------------------------------------------------
!
      integer N_sl
      !parameter (N_sl=40)
      parameter (N_sl=0)

!
!----------------------------------------------------------------------
!  related variables
!----------------------------------------------------------------------
!
      integer Lmmpi,Mmmpi,iminmpi,imaxmpi,jminmpi,jmaxmpi
      common /comm_setup_mpi1/ Lmmpi,Mmmpi
      common /comm_setup_mpi2/ iminmpi,imaxmpi,jminmpi,jmaxmpi
!
! Domain subdivision parameters
! ====== =========== ==========
!
! NPP            Maximum allowed number of parallel threads;
! NSUB_X,NSUB_E  Number of SHARED memory subdomains in XI- and
!                                                ETA-directions;
! NNODES        Total number of  processes (nodes);
! NP_XI,NP_ETA  Number of  subdomains in XI- and ETA-directions;
!
      integer NSUB_X, NSUB_E, NPP
      integer NP_XI, NP_ETA, NNODES
      parameter (NP_XI=1,  NP_ETA=4,  NNODES=NP_XI*NP_ETA)
      parameter (NPP=1)
      parameter (NSUB_X=1, NSUB_E=1)

!
!----------------------------------------------------------------------
! Number maximum of weights for the barotropic mode
!----------------------------------------------------------------------
!
      integer NWEIGHT
      parameter (NWEIGHT=1000)

!
!----------------------------------------------------------------------
! Tides
!----------------------------------------------------------------------
!
!
!----------------------------------------------------------------------
! Wetting-Drying
!----------------------------------------------------------------------
!
!
!----------------------------------------------------------------------
! Minimum water depth above which wave forcing is applied
! (D_wavedry>D_wetdry if WET_DRY is activated)
!----------------------------------------------------------------------
!
!----------------------------------------------------------------------
! Point sources, Floast, Stations
!----------------------------------------------------------------------
!

!
!----------------------------------------------------------------------
! Derived dimension parameters
!----------------------------------------------------------------------
!
      integer stdout, Np, NpHz, padd_X,padd_E
      parameter (stdout=6)
      parameter (Np=N+1)
      parameter (NpHz=(N+1+N_sl))
      parameter (Lm=(LLm+NP_XI-1)/NP_XI, Mm=(MMm+NP_ETA-1)/NP_ETA)
      parameter (padd_X=(Lm+2)/2-(Lm+1)/2)
      parameter (padd_E=(Mm+2)/2-(Mm+1)/2)

      integer NSA, N2d,N3d,N3dHz, size_XI,size_ETA
      integer se,sse, sz,ssz
      parameter (NSA=28)
      parameter (size_XI=7+(Lm+NSUB_X-1)/NSUB_X)
      parameter (size_ETA=7+(Mm+NSUB_E-1)/NSUB_E)
      parameter (sse=size_ETA/Np, ssz=Np/size_ETA)
      parameter (se=sse/(sse+ssz), sz=1-se)
      parameter (N2d=size_XI*(se*size_ETA+sz*Np))
      parameter (N3d=size_XI*size_ETA*Np)
      parameter (N3dHz=size_XI*size_ETA*NpHz)

!
!----------------------------------------------------------------------
! I/O : flag for type sigma vertical transformation
!----------------------------------------------------------------------
!
      real Vtransform
      parameter (Vtransform=2)

!
!----------------------------------------------------------------------
! Number of tracers
!----------------------------------------------------------------------
!
      integer   NT, NTA, itemp, NTot
      integer   ntrc_temp, ntrc_salt, ntrc_pas, ntrc_bio, ntrc_sed
      integer   ntrc_subs, ntrc_substot
!
      parameter (itemp=1)
      parameter (ntrc_temp=1)
      parameter (ntrc_salt=1)
      parameter (ntrc_pas=0)
      parameter (ntrc_bio=0)


!
      parameter (ntrc_subs=0, ntrc_substot=0)

!
      parameter (ntrc_sed=0)
!
! Total number of active tracers
!
      parameter (NTA=itemp+ntrc_salt)

!
! Total number of tracers
!
      parameter (NT=itemp+ntrc_salt+ntrc_pas+ntrc_bio+ntrc_sed)
      parameter (NTot=NT)





!
!----------------------------------------------------------------------
! Tracer identification indices
!----------------------------------------------------------------------
!
      integer   ntrc_diats, ntrc_diauv, ntrc_diabio
      integer   ntrc_diavrt, ntrc_diaek, ntrc_diapv
      integer   ntrc_diaeddy, ntrc_surf
     &          , isalt
!


!
! ================  Parameters  =====================
!

      parameter (isalt=itemp+1)

!
! ===  BIOLOGY  ===
!
      parameter (ntrc_diabio=0)

!
! === SEDIMENTS ===
!


!
! ===  u,v and tracer equations Diagnostics  ===
!
      parameter (ntrc_diats=0)
      parameter (ntrc_diauv=0)
      parameter (ntrc_diavrt=0)
      parameter (ntrc_diaek=0)
      parameter (ntrc_diapv=0)
      parameter (ntrc_diaeddy=0)
      parameter (ntrc_surf=0)

!
!----------------------------------------------------------------------
! Max time increment for computing bottom stress at the 3D fast time
! steps
!----------------------------------------------------------------------
!
!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!
! This is include file "scalars.h"
!---------------------------------
!
!  The following common block contains time variables and indices
! for 2D (k-indices) and 3D (n-indices) computational engines. Since
! they are changed together, they are placed into the same cache line
! despite their mixed type, so that only one cachene is being
! invalidated and has to be propagated accross the cluster.
!
! Note that the real values are placed first into the common block
! before the integer variables. This is done to prevent the
! possibility of misallignment of the 8-byte objects in the case
! when an uneven number of 4-byte integers is placed before a 8-byte
! real (in the case when default real size is set to 8bytes).
! Thought misallignment is not formally a violation of fortran
! standard, it may cause performance degradation and/or make compiler
! issue a warning message (Sun, DEC Alpha) or even crash (Alpha).
!
! time        Time since initialization [seconds];
! time_start  Initialization time [seconds];
! tdays       Time since initialization [days];
! dt          Time step for 3D primitive equations [seconds];
! dtfast      Time step for 2D (barotropic) mode [seconds];
!
      real dt, dtfast, time, time2, time_start, tdays, start_time
      integer ndtfast, iic, kstp, krhs, knew, next_kstp
     &      , iif, nstp, nrhs, nnew, nbstep3d

      logical PREDICTOR_2D_STEP
      common /time_indices/  dt,dtfast, time, time2,time_start, tdays,
     &     ndtfast, iic, kstp, krhs, knew, next_kstp,
     &     start_time,
     &                       iif, nstp, nrhs, nnew, nbstep3d,

     &                       PREDICTOR_2D_STEP

!
! Slowly changing variables: these are typically set in the beginning
! of the run and either remain unchanged, or are changing only in
! association with the I/0.
!
! xl, el   Physical size (m) of domain box in the XI-,ETA-directions.
!
! Tcline   Width (m) of surface or bottom boundary layer in which
!          higher vertical resolution is required during stretching.
! theta_s  S-coordinate surface control parameter, [0<theta_s<20].
! theta_b  S-coordinate bottom control parameter, [0<theta_b<1].
! hc       S-coordinate parameter, hc=min(hmin,Tcline).
!
! sc_r     S-coordinate independent variable, [-1 < sc < 0] at
!             vertical RHO-points
! sc_w     S-coordinate independent variable, [-1 < sc < 0] at
!             vertical W-points.
! Cs_r     Set of S-curves used to stretch the vertical coordinate
!             lines that follow the topography at vertical RHO-points.
! Cs_w     Set of S-curves used to stretch the vertical coordinate
!             lines that follow the topography at vertical W-points.
!
! rho0     Boussinesque Approximation Mean density [kg/m^3].
! R0       Background constant density anomaly [kg/m^3] used in
!                                      linear equation of state.
! T0,S0    Background temperature (Celsius) and salinity [PSU]
!                          values used in analytical fields;
! Tcoef    Thermal expansion coefficient in linear EOS;
! Scoef    Saline contraction coefficient in linear EOS;
!
! rdrg     Linear bottom drag coefficient.
! rdrg2    Quadratic bottom drag coefficient.
! Cdb_max  Maximum bottom drag coefficient allowed.
! Cdb_min  Minimum bottom drag coefficient to avoid the
!                law-of-the-wall to extend indefinitely.
! Zobt      Bottom roughness (m).
!
! gamma2   Slipperiness parameter, either 1. (free-slip)
!
! ntstart  Starting timestep in evolving the 3D primitive equations;
!                              usually 1, if not a restart run.
! ntimes   Number of timesteps for the 3D primitive equations in
!                                                    the current run.
! ndtfast  Number of timesteps for 2-D equations between each "dt".
!
! nrst     Number of timesteps between storage of restart fields.
! nwrt     Number of timesteps between writing of fields into
!                                                     history file.
! ninfo    Number of timesteps between print of single line
!                                   information to standard output.
! nsta     Number of timesteps between storage of station data.
! navg     Number of timesteps between storage of time-averaged
!                                                           fields.
! ntsavg   Starting timestep for accumulation of output time-
!                                                 averaged fields.
! nrrec    Counter of restart time records to read from disk,
!                   the last is used as the initial conditions.
!
! ldefhis  Logical switch used to create the history file.
!             If TRUE, a new history file is created. If FALSE,
!             data is appended to an existing history file.
! levsfrc  Deepest level to apply surface momentum stress as
!                                                 bodyforce.
! levbfrc  Shallowest level to apply bottom momentum stress as
!                                                 bodyforce.
! got_tini Logical switch used at initialisation
!              If TRUE, the tracer is present in the initial file
!              If FALSE, the tracer needs an analytical value
!
! got_inised Logical switch used at initialisation  of sediments
!              If TRUE, the sediment var. is in the initial file
!              If FALSE, the sed. var. gets analytical value from file
!
! got_inibed Logical switch used at initialisation of ripple height, length
!              If TRUE, the ripple var. is in the initial file
!              If FALSE, the ripple var. is obtained from file (ifdef also SEDIMENT)
!                        the ripple var. is set in ana_bsedim (ifndef SEDIMENT)
!
      real time_avg, time2_avg, rho0
     &               , rdrg, rdrg2, Cdb_min, Cdb_max, Zobt
     &               , xl, el, visc2, visc4, gamma2
      real  theta_s,   theta_b,   Tcline,  hc
      real  sc_w(0:N), Cs_w(0:N), sc_r(N), Cs_r(N)
      real  rx0, rx1
      real  tnu2(NT),tnu4(NT)
      real weight(6,0:NWEIGHT)

      real  x_sponge,   v_sponge
       real  tauT_in, tauT_out, tauM_in, tauM_out
      integer numthreads,     ntstart,   ntimes,  ninfo
     &      , nfast,  nrrec,     nrst,    nwrt
     &                                 , ntsavg,  navg

      logical ldefhis
      logical got_tini(NT)

      common /scalars_main/
     &             time_avg, time2_avg,  rho0,      rdrg,    rdrg2
     &           , Zobt,       Cdb_min,   Cdb_max
     &           , xl, el,    visc2,     visc4,   gamma2
     &           , theta_s,   theta_b,   Tcline,  hc
     &           , sc_w,      Cs_w,      sc_r,    Cs_r
     &           , rx0,       rx1
     &           ,       tnu2,    tnu4
     &                      , weight
     &                      , x_sponge,   v_sponge
     &                      , tauT_in, tauT_out, tauM_in, tauM_out
     &      , numthreads,     ntstart,   ntimes,  ninfo
     &      , nfast,  nrrec,     nrst,    nwrt
     &                                 , ntsavg,  navg
     &                      , got_tini
     &                      , ldefhis

!
!-----------------------------------------------------------------------
! This following common block contains a set of globally accessable
! variables in order to allow information exchange between parallel
! threads working on different subdomains.
!
! Global summation variables are declared with 16 byte precision
! to avoid accumulation of roundoff errors, since roundoff error
! depends on the order of summation, which is undeterministic in
! the case of summation between the parallel threads; not doing so
! would make it impossible to pass an ETALON CHECK test if there is
! a feedback of these sums into the dynamics of the model, such as
! in the case when global mass conservation is enforced.
!
!  One sunny spring day, sometime in 1989 an american tourist, who
! happened to be an attorney, was walking along a Moscow street.
! Because it was the period of 'Perestroika' (which literally means
! 'remodelling'), so that a lot of construction was going on in
! Moscow, dozens of holes and trenches were open on the street. He
! felt into one of them, broke his leg, ended up in a hospital and
! complaining: In my country if a construction firm would not place
! little red flags around the construction zone to warn passers-by
! about the danger, I will sue em for their negligence! The doctor,
! who was performing surgery on his leg replied to him: Did not you
! see the one big red flag above the whole country in the first place?
!
! WARNING: FRAGILE ALIGNMENT SEQUENCE: In the following common block:
! since real objects are grouped in pairs and integer*4 are grouped
! in quartets, it is guaranteed that 16 Byte objects are aligned
! in 16 Byte boundaries and 8 Byte objects are aligned in 8 Byte
! boundaries. Removing or introduction of variables with violation
! of parity, as well as changing the sequence of variables in the
! common block may cause violation of alignment.
!-----------------------------------------------------------------------
!
      logical synchro_flag
      common /sync_flag/ synchro_flag

      integer may_day_flag  ! This is a shared variable among nested grids
      integer tile_count, first_time, bc_count
      common /communicators_i/
     &        may_day_flag, tile_count, first_time, bc_count

      real hmin, hmax, grdmin, grdmax, Cu_min, Cu_max
      common /communicators_r/
     &     hmin, hmax, grdmin, grdmax, Cu_min, Cu_max

      real lonmin, lonmax, latmin, latmax
      common /communicators_lonlat/
     &     lonmin, lonmax, latmin, latmax

      real*8 Cu_Adv3d,  Cu_W, Cu_Nbq_X, Cu_Nbq_Y, Cu_Nbq_Z
      integer i_cx_max, j_cx_max, k_cx_max
      common /diag_vars/ Cu_Adv3d,  Cu_W,
     &        i_cx_max, j_cx_max, k_cx_max
      real*8 volume, avgke, avgpe, avgkp, bc_crss


      common /communicators_rq/
     &          volume, avgke, avgpe, avgkp, bc_crss

!
!  The following common block contains process counters and model
! timers. These are used to measure CPU time consumed by different
! parallel threads during the whole run, as well as in various
! parallel regions, if so is needed. These variables are used purely
! for diagnostic/performance measurements purposes and do not affect
! the model results.
!
      real*4 CPU_time(0:31,0:NPP)
      integer proc(0:31,0:NPP),trd_count
      common /timers_roms/CPU_time,proc,trd_count

!
!  related variables
! === ====== =========
!
      logical EAST_INTER2, WEST_INTER2, NORTH_INTER2, SOUTH_INTER2
      logical EAST_INTER, WEST_INTER, NORTH_INTER, SOUTH_INTER
      logical CORNER_SW,CORNER_NW,CORNER_NE,CORNER_SE
      integer mynode, mynode2, ii,jj, p_W,p_E,p_S,p_N, p_SW,p_SE,
     & p_NW,p_NE,NNODES2
      common /comm_setup/ mynode, mynode2, ii,jj, p_W,p_E,p_S,p_N,
     & p_SW,p_SE, p_NW,p_NE, EAST_INTER, WEST_INTER, NORTH_INTER,
     & SOUTH_INTER, EAST_INTER2, WEST_INTER2, NORTH_INTER2, SOUTH_INTER2,
     & CORNER_SW,CORNER_NW,CORNER_NE,CORNER_SE,NNODES2


!
! Physical constants:
! ======== ==========

      real pi, deg2rad, rad2deg
      parameter (pi=3.14159265358979323846, deg2rad=pi/180.,
     &                                      rad2deg=180./pi)
!
! Earth radius [m]; Earth rotation [rad/s]; Acceleration of gravity [m/s^2];
! duration of the day in seconds and its inverse; Julian offset day.

      real Eradius, Erotation, g, day2sec,sec2day, jul_off,
     &     year2day,day2year
      parameter (Eradius=6371315.0,  Erotation=7.292115090e-5,
     &           day2sec=86400., sec2day=1./86400.,
     &           year2day=365.25, day2year=1./365.25,
     &           jul_off=2440000.)
!
! Acceleration of gravity (nondimensional for Soliton problem)
!
      parameter (g=9.81)
!
!  Specific heat [Joules/kg/degC] for seawater, it is approximately
!  4000, and varies only slightly (see Gill, 1982, Appendix 3).
!
      real Cp
      parameter (Cp=3985.0)

      real vonKar
      parameter (vonKar=0.41)
!
!   FillValue (Needed if the FILLVAL key is defined)
!   (See fillvalue.F subroutine)
      real spval
      parameter (spval=-999.0)
      logical mask_val
      parameter (mask_val = .true.)
!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!
!$AGRIF_DO_NOT_TREAT
      INTEGER :: ocean_grid_comm
      common /cpl_comm/ ocean_grid_comm
!$AGRIF_END_DO_NOT_TREAT


      include 'mpif.h'
      logical, parameter :: use_host_device = .TRUE.
      integer sub_X,size_X, sub_E,size_E   
      parameter (sub_X=Lm,  size_X=3*(sub_X+2*3)-1,
     &           sub_E=Mm,  size_E=3*(sub_E+2*3)-1)

      real ibuf_sndN(0:size_X), ibuf_revN(0:size_X),
     &     ibuf_sndS(0:size_X), ibuf_revS(0:size_X),
     &     jbuf_sndW(0:size_E), jbuf_sndE(0:size_E),
     &     jbuf_revW(0:size_E), jbuf_revE(0:size_E)

      real buf_snd4(3*3),     buf_snd2(3*3),
     &     buf_rev4(3*3),     buf_rev2(3*3),
     &     buf_snd1(3*3),     buf_snd3(3*3), 
     &     buf_rev1(3*3),     buf_rev3(3*3)

      common/buf_mpi/buf_snd4,buf_snd2,buf_rev4,buf_rev2,
     &               buf_snd1,buf_snd3,buf_rev1,buf_rev3,
     &     ibuf_sndN, ibuf_revN,
     &     ibuf_sndS, ibuf_revS,
     &     jbuf_sndW, jbuf_sndE,
     &     jbuf_revW, jbuf_revE


      integer size_Z,sub_X_3D,size_X_3D, sub_E_3D,size_E_3D

      parameter (size_Z=3*3*(N+1),
     &       sub_X_3D=(Lm+NSUB_X-1)/NSUB_X,
     &     size_X_3D=(N+1)*3*(sub_X_3D+2*3),
     &     sub_E_3D=(Mm+NSUB_E-1)/NSUB_E,
     &     size_E_3D=(N+1)*3*(sub_E_3D+2*3))

      real ibuf_sndN_3D(size_X_3D), ibuf_revN_3D(size_X_3D),
     &     ibuf_sndS_3D(size_X_3D), ibuf_revS_3D(size_X_3D),
     &     jbuf_sndW_3D(size_E_3D), jbuf_sndE_3D(size_E_3D),
     &     jbuf_revW_3D(size_E_3D), jbuf_revE_3D(size_E_3D)
      real buf_snd4_3D(size_Z), buf_snd2_3D(size_Z),
     &     buf_rev4_3D(size_Z), buf_rev2_3D(size_Z),
     &     buf_snd1_3D(size_Z), buf_snd3_3D(size_Z), 
     &     buf_rev1_3D(size_Z), buf_rev3_3D(size_Z)

      common/buf_mpi_3D/buf_snd4_3D, buf_snd2_3D,
     &                  buf_rev4_3D, buf_rev2_3D,
     &                  buf_snd1_3D, buf_snd3_3D,
     &                  buf_rev1_3D, buf_rev3_3D,
     &     ibuf_sndN_3D, ibuf_revN_3D,
     &     ibuf_sndS_3D, ibuf_revS_3D,
     &     jbuf_sndW_3D, jbuf_sndE_3D,
     &     jbuf_revW_3D, jbuf_revE_3D
!
! Nb of boundary points involved in communication
!
      integer Npts,ipts,jpts
      parameter (Npts=2)
      integer nmax
      real A(-1:Lm+2+padd_X,-1:Mm+2+padd_E,nmax)
CSDISTRIBUTE_RESHAPE A(BLOCK_PATTERN) BLOCK_CLAUSE
      integer Istr,Iend,Jstr,Jend, i,j,k, isize,jsize,ksize,
     &        req(8), status(MPI_STATUS_SIZE,8), ierr
      integer iter, mdii, mdjj
!
!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!
      integer imin,imax,ishft, jmin,jmax,jshft
      if (ii.eq.0 .and. Istr.eq.1) then   ! Extra point on either
        imin=Istr-1                       ! side to accomodate ghost
      else                                ! points associated with
        imin=Istr                         ! PHYSICAL boundaries.
      endif
      if (ii.eq.NP_XI-1 .and. Iend.eq.Lmmpi) then
        imax=Iend+1
      else
        imax=Iend
      endif
      ishft=imax-imin+1

      if (jj.eq.0 .and. Jstr.eq.1) then   ! Extra point on either
        jmin=Jstr-1                       ! side to accomodate ghost
      else                                ! points associated with
        jmin=Jstr                         ! PHYSICAL boundaries.
      endif
      if (jj.eq.NP_ETA-1 .and. Jend.eq.Mmmpi) then
        jmax=Jend+1
      else
        jmax=Jend
      endif
      jshft=jmax-jmin+1


      ksize=Npts*Npts*nmax               ! message sizes for
      isize=Npts*ishft*nmax              ! corner messages and sides
      jsize=Npts*jshft*nmax              ! in XI and ETA directions
!
! Prepare to receive and send: sides....
!
                            !  Message passing split into two stages
                            !  in order to optimize Send-Recv pairing
                            !  in such a way that if one subdomain
      do iter=0,1           !  sends message to, say, its WESTERN
        mdii=mod(ii+iter,2) !  neighbor, that neighbor is preparing
        mdjj=mod(jj+iter,2) !  to receive this message first (i.e.
                            !  message coming from its EASTERN side),
                            !  rather than send his WEST
                            !  bound message, similarly to the first
                            !  subdomain.

!
! Prepare to receive and send: sides....
        if (mdii.eq.0) then
          if ((WEST_INTER2)) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do j=jmin,jmax
                do ipts=1,Npts
                  jbuf_sndW_3D(k+nmax*(j-jmin+(ipts-1)*jshft))=
     &                  A(ipts,j,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(jbuf_revW_3D,jbuf_sndW_3D)
            call MPI_Irecv (jbuf_revW_3D, jsize, MPI_DOUBLE_PRECISION,
     &                         p_W, 2, MPI_COMM_WORLD, req(1), ierr)
            call MPI_Send  (jbuf_sndW_3D, jsize, MPI_DOUBLE_PRECISION,
     &                         p_W, 1, MPI_COMM_WORLD,         ierr)
!$acc end host_data
          endif
        else
          if (EAST_INTER2) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do j=jmin,jmax
                do ipts=1,Npts
                  jbuf_sndE_3D(k+nmax*(j-jmin+(ipts-1)*jshft))=
     &                                       A(Lmmpi-Npts+ipts,j,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(jbuf_revE_3D,jbuf_sndE_3D)
            call MPI_Irecv (jbuf_revE_3D, jsize, MPI_DOUBLE_PRECISION,
     &                        p_E, 1, MPI_COMM_WORLD, req(2), ierr)
            call MPI_Send  (jbuf_sndE_3D, jsize, MPI_DOUBLE_PRECISION,
     &                        p_E, 2, MPI_COMM_WORLD,         ierr)
!$acc end host_data
          endif
        endif

        if (mdjj.eq.0) then
          if (SOUTH_INTER2) then
!$acc parallel loop if(compute_on_device) 
!$acc& default(present) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do i=imin,imax
                  ibuf_sndS_3D(k+nmax*(i-imin+(jpts-1)*ishft))=
     &                          A(i,jpts,k)
                enddo
              enddo
            enddo
!$acc update host(ibuf_sndS_3D(1:isize)) if(.not.use_host_device)
!$acc host_data if(compute_on_device.and.use_host_device)
!$acc&   use_device(ibuf_revS_3D,ibuf_sndS_3D)
            call MPI_Irecv (ibuf_revS_3D, isize, MPI_DOUBLE_PRECISION,
     &                         p_S, 4, MPI_COMM_WORLD, req(3), ierr)
            call MPI_Send  (ibuf_sndS_3D, isize, MPI_DOUBLE_PRECISION,
     &                         p_S, 3, MPI_COMM_WORLD,         ierr)
!$acc end host_data
          endif
        else
          if (NORTH_INTER2) then
!$acc parallel loop if(compute_on_device)
!$acc& default(present) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do i=imin,imax
                  ibuf_sndN_3D(k+nmax*(i-imin+(jpts-1)*ishft))=
     &                                         A(i,Mmmpi-Npts+jpts,k)
                enddo
              enddo
            enddo
!$acc update host(ibuf_sndN_3D(1:isize)) if(.not.use_host_device)
!$acc host_data if(compute_on_device.and.use_host_device)
!$acc&   use_device(ibuf_revN_3D,ibuf_sndN_3D)
            call MPI_Irecv (ibuf_revN_3D, isize, MPI_DOUBLE_PRECISION,
     &                         p_N, 3, MPI_COMM_WORLD, req(4), ierr)
            call MPI_Send  (ibuf_sndN_3D, isize, MPI_DOUBLE_PRECISION,
     &                         p_N, 4, MPI_COMM_WORLD,         ierr)
!$acc end host_data
          endif
        endif
!
! ...corners:
!
        if (mdii.eq.0) then
          if (CORNER_SW) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do ipts=1,Npts
                  buf_snd1_3D(k+nmax*(ipts-1+Npts*(jpts-1)))=
     &                 A(ipts,jpts,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(buf_rev1_3D,buf_snd1_3D)
            call MPI_Irecv (buf_rev1_3D, ksize, MPI_DOUBLE_PRECISION, p_SW,
     &                                6, MPI_COMM_WORLD, req(5),ierr)
            call MPI_Send  (buf_snd1_3D, ksize, MPI_DOUBLE_PRECISION, p_SW,
     &                                5, MPI_COMM_WORLD,        ierr)
!$acc end host_data
          endif
        else
          if (CORNER_NE) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do ipts=1,Npts
                  buf_snd2_3D(k+nmax*(ipts-1+Npts*(jpts-1)))=
     &                            A(Lmmpi+ipts-Npts,Mmmpi+jpts-Npts,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(buf_rev2_3D,buf_snd2_3D)
            call MPI_Irecv (buf_rev2_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_NE, 5, MPI_COMM_WORLD, req(6),ierr)
            call MPI_Send  (buf_snd2_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_NE, 6, MPI_COMM_WORLD,        ierr)
!$acc end host_data
          endif
        endif

        if (mdii.eq.1) then
          if (CORNER_SE) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do ipts=1,Npts
                  buf_snd3_3D(k+nmax*(ipts-1+Npts*(jpts-1)))=
     &                                A(Lmmpi+ipts-Npts,jpts,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(buf_rev3_3D,buf_snd3_3D)
            call MPI_Irecv (buf_rev3_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_SE, 8, MPI_COMM_WORLD, req(7),ierr)
            call MPI_Send  (buf_snd3_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_SE, 7, MPI_COMM_WORLD,        ierr)
!$acc end host_data
          endif
        else
          if (CORNER_NW) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do ipts=1,Npts
                  buf_snd4_3D(k+nmax*(ipts-1+Npts*(jpts-1)))=
     &                                A(ipts,Mmmpi+jpts-Npts,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(buf_rev4_3D,buf_snd4_3D)
            call MPI_Irecv (buf_rev4_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_NW, 7, MPI_COMM_WORLD, req(8),ierr)
            call MPI_Send  (buf_snd4_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_NW, 8, MPI_COMM_WORLD,        ierr)
!$acc end host_data
          endif
        endif
      enddo   !<-- iter

!
! Wait for completion of receive and fill ghost points: sides...
!
      if (WEST_INTER2) then
        call MPI_Wait (req(1),status(1,1),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do j=jmin,jmax
            do ipts=1,Npts
             A(ipts-Npts,j,k)=
     &          jbuf_revW_3D(k+nmax*(j-jmin+(ipts-1)*jshft))
            enddo
          enddo
        enddo
      endif
      if (WEST_INTER .and. .not. WEST_INTER2) then
        do k=1,nmax
          do j=jmin,jmax
            do ipts=1,Npts
              A(ipts-Npts,j,k)=A(ipts,j,k)
            enddo
          enddo
        enddo
      endif

      if (EAST_INTER2) then
        call MPI_Wait (req(2),status(1,2),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do j=jmin,jmax
            do ipts=1,Npts
              A(Lmmpi+ipts,j,k)=
     &                         jbuf_revE_3D(k+nmax*(j-jmin+(ipts-1)
     &                         *jshft))
            enddo
          enddo
        enddo
      endif
      if (EAST_INTER .and. .not. EAST_INTER2) then
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do j=jmin,jmax
            do ipts=1,Npts
              A(Lmmpi+ipts,j,k)=A(Lmmpi+ipts-Npts,j,k)
            enddo
          enddo
        enddo
      endif


      if (SOUTH_INTER2) then
        call MPI_Wait (req(3),status(1,3),ierr)
!$acc update device(ibuf_revS_3D(1:isize)) if(.not.use_host_device)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do jpts=1,Npts
            do i=imin,imax
              A(i,jpts-Npts,k)=ibuf_revS_3D(k+nmax*(i-imin+(jpts-1)
     &                        *ishft))
            enddo
          enddo
        enddo
      endif
      if (SOUTH_INTER .and. .not. SOUTH_INTER2) then
!$acc parallel loop if(compute_on_device) collapse(3) 
       do k=1,nmax
          do i=imin,imax
            do jpts=1,Npts
              A(i,jpts-Npts,k)=A(i,jpts,k)
            enddo
          enddo
        enddo
      endif

      if (NORTH_INTER2) then
        call MPI_Wait (req(4),status(1,4),ierr)
!$acc update device(ibuf_revN_3D(1:isize)) if(.not.use_host_device)
!$acc parallel loop if(compute_on_device) collapse(3)
        do k=1,nmax
          do jpts=1,Npts
            do i=imin,imax
              A(i,Mmmpi+jpts,k)=
     &          ibuf_revN_3D(k+nmax*(i-imin+(jpts-1)*ishft))
            enddo
          enddo
        enddo
      endif
      if (NORTH_INTER .and. .not. NORTH_INTER2) then
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do i=imin,imax
            do jpts=1,Npts
              A(i,Mmmpi+jpts,k)=A(i,Mmmpi+jpts-Npts,k)
            enddo
          enddo
        enddo
      endif
!
! ...corners:
!
      if (CORNER_SW) then
        call MPI_Wait (req(5),status(1,5),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(ipts-Npts,jpts-Npts,k)=
     &                           buf_rev1_3D(k+nmax*(ipts-1+Npts*(jpts-1)))
            enddo
          enddo
        enddo
      endif
      if (.not. CORNER_SW  .and.
     &   SOUTH_INTER .and.  WEST_INTER ) then
!$acc parallel loop if(compute_on_device) collapse(3) 
       do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(ipts-Npts,jpts-Npts,k)=
     &                           A(ipts,jpts,k)
            enddo
          enddo
        enddo
       endif

      if (CORNER_NE) then
        call MPI_Wait (req(6),status(1,6),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(Lmmpi+ipts,Mmmpi+jpts,k)=
     &                           buf_rev2_3D(k+nmax*(ipts-1+Npts*(jpts-1)))
            enddo
          enddo
        enddo
      endif
      if (.not. CORNER_NE  .and.
     &   NORTH_INTER .and.  EAST_INTER ) then
!$acc parallel loop if(compute_on_device) collapse(3) 
       do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(Lmmpi+ipts,Mmmpi+jpts,k)=
     &              A(Lmmpi+ipts-Npts,Mmmpi+jpts-Npts,k)
            enddo
          enddo
        enddo
       endif


      if (CORNER_SE) then
        call MPI_Wait (req(7),status(1,7),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(Lmmpi+ipts,jpts-Npts,k)=
     &                           buf_rev3_3D(k+nmax*(ipts-1+Npts*(jpts-1)))
            enddo
          enddo
        enddo
      endif
      if (.not. CORNER_SE .and.
     &   SOUTH_INTER .and.  EAST_INTER ) then
!$acc parallel loop if(compute_on_device) collapse(3)
       do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
               A(Lmmpi+ipts,jpts-Npts,k)=
     &                           A(Lmmpi+ipts-Npts,jpts,k)
            enddo
          enddo
        enddo
       endif


      if (CORNER_NW) then
        call MPI_Wait (req(8),status(1,8),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(ipts-Npts,Mmmpi+jpts,k)=
     &                           buf_rev4_3D(k+nmax*(ipts-1+Npts*(jpts-1)))
            enddo
          enddo
        enddo
      endif
      if (.not. CORNER_NW  .and.
     &   NORTH_INTER .and.  WEST_INTER ) then
!$acc parallel loop if(compute_on_device) collapse(3) 
       do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(ipts-Npts,Mmmpi+jpts,k)=A(ipts,Mmmpi+jpts-Npts,k)
            enddo
          enddo
        enddo
       endif


      return
      end

!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!







                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      

                      
                      
                      
                      
                      
                      

                      

                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                     
                      
!    RVTK test (Restartability or Parallel reproducibility)








                      
                      
                      
                      
                      


!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!






















































































































!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!





















!









!-# define float dfloat
!-# define FLoaT dfloat
!-# define FLOAT dfloat
!-# define sqrt dsqrt
!-# define SQRT dsqrt
!-# define exp dexp
!-# define EXP dexp
!-# define dtanh dtanh
!-# define TANH dtanh









      subroutine MessPass3D_3pts_tile (Istr,Iend,Jstr,Jend, A, nmax)
!
!======================================================================
!
! This subroutine is designed for CROCO- code. It exchanges domain
! boundary information, including 2 (or 3) ghost-cells in each
! direction.
!
! Ping Wang 9/15/99.
! Patrick Marchesiello 2012: generic number of ghost-cells Npts
!
!======================================================================
!
!     implicit none
!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!
!----------------------------------------------------------------------
! Dimensions of Physical Grid and array dimensions
!----------------------------------------------------------------------
!
! LLm,MMm  Number of the internal points of the PHYSICAL grid.
!          in the XI- and ETA-directions [physical side boundary
!          points and peroodic ghost points (if any) are excluded].
!
! Lm,Mm    Number of the internal points [see above] of array
!          covering a Message Passing subdomain. In the case when
!          no Message Passing partitioning is used, these two are
!          the same as LLm,MMm.
!
! N        Number of vertical levels.
!
      integer  LLm,Lm,MMm,Mm,N, LLm0,MMm0
      parameter (LLm0=41,   MMm0=42,   N=32)   ! 

      parameter (LLm=LLm0,  MMm=MMm0)

!
!----------------------------------------------------------------------
! Number of layers in Sediment (SL)
!----------------------------------------------------------------------
!
      integer N_sl
      !parameter (N_sl=40)
      parameter (N_sl=0)

!
!----------------------------------------------------------------------
!  related variables
!----------------------------------------------------------------------
!
      integer Lmmpi,Mmmpi,iminmpi,imaxmpi,jminmpi,jmaxmpi
      common /comm_setup_mpi1/ Lmmpi,Mmmpi
      common /comm_setup_mpi2/ iminmpi,imaxmpi,jminmpi,jmaxmpi
!
! Domain subdivision parameters
! ====== =========== ==========
!
! NPP            Maximum allowed number of parallel threads;
! NSUB_X,NSUB_E  Number of SHARED memory subdomains in XI- and
!                                                ETA-directions;
! NNODES        Total number of  processes (nodes);
! NP_XI,NP_ETA  Number of  subdomains in XI- and ETA-directions;
!
      integer NSUB_X, NSUB_E, NPP
      integer NP_XI, NP_ETA, NNODES
      parameter (NP_XI=1,  NP_ETA=4,  NNODES=NP_XI*NP_ETA)
      parameter (NPP=1)
      parameter (NSUB_X=1, NSUB_E=1)

!
!----------------------------------------------------------------------
! Number maximum of weights for the barotropic mode
!----------------------------------------------------------------------
!
      integer NWEIGHT
      parameter (NWEIGHT=1000)

!
!----------------------------------------------------------------------
! Tides
!----------------------------------------------------------------------
!
!
!----------------------------------------------------------------------
! Wetting-Drying
!----------------------------------------------------------------------
!
!
!----------------------------------------------------------------------
! Minimum water depth above which wave forcing is applied
! (D_wavedry>D_wetdry if WET_DRY is activated)
!----------------------------------------------------------------------
!
!----------------------------------------------------------------------
! Point sources, Floast, Stations
!----------------------------------------------------------------------
!

!
!----------------------------------------------------------------------
! Derived dimension parameters
!----------------------------------------------------------------------
!
      integer stdout, Np, NpHz, padd_X,padd_E
      parameter (stdout=6)
      parameter (Np=N+1)
      parameter (NpHz=(N+1+N_sl))
      parameter (Lm=(LLm+NP_XI-1)/NP_XI, Mm=(MMm+NP_ETA-1)/NP_ETA)
      parameter (padd_X=(Lm+2)/2-(Lm+1)/2)
      parameter (padd_E=(Mm+2)/2-(Mm+1)/2)

      integer NSA, N2d,N3d,N3dHz, size_XI,size_ETA
      integer se,sse, sz,ssz
      parameter (NSA=28)
      parameter (size_XI=7+(Lm+NSUB_X-1)/NSUB_X)
      parameter (size_ETA=7+(Mm+NSUB_E-1)/NSUB_E)
      parameter (sse=size_ETA/Np, ssz=Np/size_ETA)
      parameter (se=sse/(sse+ssz), sz=1-se)
      parameter (N2d=size_XI*(se*size_ETA+sz*Np))
      parameter (N3d=size_XI*size_ETA*Np)
      parameter (N3dHz=size_XI*size_ETA*NpHz)

!
!----------------------------------------------------------------------
! I/O : flag for type sigma vertical transformation
!----------------------------------------------------------------------
!
      real Vtransform
      parameter (Vtransform=2)

!
!----------------------------------------------------------------------
! Number of tracers
!----------------------------------------------------------------------
!
      integer   NT, NTA, itemp, NTot
      integer   ntrc_temp, ntrc_salt, ntrc_pas, ntrc_bio, ntrc_sed
      integer   ntrc_subs, ntrc_substot
!
      parameter (itemp=1)
      parameter (ntrc_temp=1)
      parameter (ntrc_salt=1)
      parameter (ntrc_pas=0)
      parameter (ntrc_bio=0)


!
      parameter (ntrc_subs=0, ntrc_substot=0)

!
      parameter (ntrc_sed=0)
!
! Total number of active tracers
!
      parameter (NTA=itemp+ntrc_salt)

!
! Total number of tracers
!
      parameter (NT=itemp+ntrc_salt+ntrc_pas+ntrc_bio+ntrc_sed)
      parameter (NTot=NT)





!
!----------------------------------------------------------------------
! Tracer identification indices
!----------------------------------------------------------------------
!
      integer   ntrc_diats, ntrc_diauv, ntrc_diabio
      integer   ntrc_diavrt, ntrc_diaek, ntrc_diapv
      integer   ntrc_diaeddy, ntrc_surf
     &          , isalt
!


!
! ================  Parameters  =====================
!

      parameter (isalt=itemp+1)

!
! ===  BIOLOGY  ===
!
      parameter (ntrc_diabio=0)

!
! === SEDIMENTS ===
!


!
! ===  u,v and tracer equations Diagnostics  ===
!
      parameter (ntrc_diats=0)
      parameter (ntrc_diauv=0)
      parameter (ntrc_diavrt=0)
      parameter (ntrc_diaek=0)
      parameter (ntrc_diapv=0)
      parameter (ntrc_diaeddy=0)
      parameter (ntrc_surf=0)

!
!----------------------------------------------------------------------
! Max time increment for computing bottom stress at the 3D fast time
! steps
!----------------------------------------------------------------------
!
!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!
! This is include file "scalars.h"
!---------------------------------
!
!  The following common block contains time variables and indices
! for 2D (k-indices) and 3D (n-indices) computational engines. Since
! they are changed together, they are placed into the same cache line
! despite their mixed type, so that only one cachene is being
! invalidated and has to be propagated accross the cluster.
!
! Note that the real values are placed first into the common block
! before the integer variables. This is done to prevent the
! possibility of misallignment of the 8-byte objects in the case
! when an uneven number of 4-byte integers is placed before a 8-byte
! real (in the case when default real size is set to 8bytes).
! Thought misallignment is not formally a violation of fortran
! standard, it may cause performance degradation and/or make compiler
! issue a warning message (Sun, DEC Alpha) or even crash (Alpha).
!
! time        Time since initialization [seconds];
! time_start  Initialization time [seconds];
! tdays       Time since initialization [days];
! dt          Time step for 3D primitive equations [seconds];
! dtfast      Time step for 2D (barotropic) mode [seconds];
!
      real dt, dtfast, time, time2, time_start, tdays, start_time
      integer ndtfast, iic, kstp, krhs, knew, next_kstp
     &      , iif, nstp, nrhs, nnew, nbstep3d

      logical PREDICTOR_2D_STEP
      common /time_indices/  dt,dtfast, time, time2,time_start, tdays,
     &     ndtfast, iic, kstp, krhs, knew, next_kstp,
     &     start_time,
     &                       iif, nstp, nrhs, nnew, nbstep3d,

     &                       PREDICTOR_2D_STEP

!
! Slowly changing variables: these are typically set in the beginning
! of the run and either remain unchanged, or are changing only in
! association with the I/0.
!
! xl, el   Physical size (m) of domain box in the XI-,ETA-directions.
!
! Tcline   Width (m) of surface or bottom boundary layer in which
!          higher vertical resolution is required during stretching.
! theta_s  S-coordinate surface control parameter, [0<theta_s<20].
! theta_b  S-coordinate bottom control parameter, [0<theta_b<1].
! hc       S-coordinate parameter, hc=min(hmin,Tcline).
!
! sc_r     S-coordinate independent variable, [-1 < sc < 0] at
!             vertical RHO-points
! sc_w     S-coordinate independent variable, [-1 < sc < 0] at
!             vertical W-points.
! Cs_r     Set of S-curves used to stretch the vertical coordinate
!             lines that follow the topography at vertical RHO-points.
! Cs_w     Set of S-curves used to stretch the vertical coordinate
!             lines that follow the topography at vertical W-points.
!
! rho0     Boussinesque Approximation Mean density [kg/m^3].
! R0       Background constant density anomaly [kg/m^3] used in
!                                      linear equation of state.
! T0,S0    Background temperature (Celsius) and salinity [PSU]
!                          values used in analytical fields;
! Tcoef    Thermal expansion coefficient in linear EOS;
! Scoef    Saline contraction coefficient in linear EOS;
!
! rdrg     Linear bottom drag coefficient.
! rdrg2    Quadratic bottom drag coefficient.
! Cdb_max  Maximum bottom drag coefficient allowed.
! Cdb_min  Minimum bottom drag coefficient to avoid the
!                law-of-the-wall to extend indefinitely.
! Zobt      Bottom roughness (m).
!
! gamma2   Slipperiness parameter, either 1. (free-slip)
!
! ntstart  Starting timestep in evolving the 3D primitive equations;
!                              usually 1, if not a restart run.
! ntimes   Number of timesteps for the 3D primitive equations in
!                                                    the current run.
! ndtfast  Number of timesteps for 2-D equations between each "dt".
!
! nrst     Number of timesteps between storage of restart fields.
! nwrt     Number of timesteps between writing of fields into
!                                                     history file.
! ninfo    Number of timesteps between print of single line
!                                   information to standard output.
! nsta     Number of timesteps between storage of station data.
! navg     Number of timesteps between storage of time-averaged
!                                                           fields.
! ntsavg   Starting timestep for accumulation of output time-
!                                                 averaged fields.
! nrrec    Counter of restart time records to read from disk,
!                   the last is used as the initial conditions.
!
! ldefhis  Logical switch used to create the history file.
!             If TRUE, a new history file is created. If FALSE,
!             data is appended to an existing history file.
! levsfrc  Deepest level to apply surface momentum stress as
!                                                 bodyforce.
! levbfrc  Shallowest level to apply bottom momentum stress as
!                                                 bodyforce.
! got_tini Logical switch used at initialisation
!              If TRUE, the tracer is present in the initial file
!              If FALSE, the tracer needs an analytical value
!
! got_inised Logical switch used at initialisation  of sediments
!              If TRUE, the sediment var. is in the initial file
!              If FALSE, the sed. var. gets analytical value from file
!
! got_inibed Logical switch used at initialisation of ripple height, length
!              If TRUE, the ripple var. is in the initial file
!              If FALSE, the ripple var. is obtained from file (ifdef also SEDIMENT)
!                        the ripple var. is set in ana_bsedim (ifndef SEDIMENT)
!
      real time_avg, time2_avg, rho0
     &               , rdrg, rdrg2, Cdb_min, Cdb_max, Zobt
     &               , xl, el, visc2, visc4, gamma2
      real  theta_s,   theta_b,   Tcline,  hc
      real  sc_w(0:N), Cs_w(0:N), sc_r(N), Cs_r(N)
      real  rx0, rx1
      real  tnu2(NT),tnu4(NT)
      real weight(6,0:NWEIGHT)

      real  x_sponge,   v_sponge
       real  tauT_in, tauT_out, tauM_in, tauM_out
      integer numthreads,     ntstart,   ntimes,  ninfo
     &      , nfast,  nrrec,     nrst,    nwrt
     &                                 , ntsavg,  navg

      logical ldefhis
      logical got_tini(NT)

      common /scalars_main/
     &             time_avg, time2_avg,  rho0,      rdrg,    rdrg2
     &           , Zobt,       Cdb_min,   Cdb_max
     &           , xl, el,    visc2,     visc4,   gamma2
     &           , theta_s,   theta_b,   Tcline,  hc
     &           , sc_w,      Cs_w,      sc_r,    Cs_r
     &           , rx0,       rx1
     &           ,       tnu2,    tnu4
     &                      , weight
     &                      , x_sponge,   v_sponge
     &                      , tauT_in, tauT_out, tauM_in, tauM_out
     &      , numthreads,     ntstart,   ntimes,  ninfo
     &      , nfast,  nrrec,     nrst,    nwrt
     &                                 , ntsavg,  navg
     &                      , got_tini
     &                      , ldefhis

!
!-----------------------------------------------------------------------
! This following common block contains a set of globally accessable
! variables in order to allow information exchange between parallel
! threads working on different subdomains.
!
! Global summation variables are declared with 16 byte precision
! to avoid accumulation of roundoff errors, since roundoff error
! depends on the order of summation, which is undeterministic in
! the case of summation between the parallel threads; not doing so
! would make it impossible to pass an ETALON CHECK test if there is
! a feedback of these sums into the dynamics of the model, such as
! in the case when global mass conservation is enforced.
!
!  One sunny spring day, sometime in 1989 an american tourist, who
! happened to be an attorney, was walking along a Moscow street.
! Because it was the period of 'Perestroika' (which literally means
! 'remodelling'), so that a lot of construction was going on in
! Moscow, dozens of holes and trenches were open on the street. He
! felt into one of them, broke his leg, ended up in a hospital and
! complaining: In my country if a construction firm would not place
! little red flags around the construction zone to warn passers-by
! about the danger, I will sue em for their negligence! The doctor,
! who was performing surgery on his leg replied to him: Did not you
! see the one big red flag above the whole country in the first place?
!
! WARNING: FRAGILE ALIGNMENT SEQUENCE: In the following common block:
! since real objects are grouped in pairs and integer*4 are grouped
! in quartets, it is guaranteed that 16 Byte objects are aligned
! in 16 Byte boundaries and 8 Byte objects are aligned in 8 Byte
! boundaries. Removing or introduction of variables with violation
! of parity, as well as changing the sequence of variables in the
! common block may cause violation of alignment.
!-----------------------------------------------------------------------
!
      logical synchro_flag
      common /sync_flag/ synchro_flag

      integer may_day_flag  ! This is a shared variable among nested grids
      integer tile_count, first_time, bc_count
      common /communicators_i/
     &        may_day_flag, tile_count, first_time, bc_count

      real hmin, hmax, grdmin, grdmax, Cu_min, Cu_max
      common /communicators_r/
     &     hmin, hmax, grdmin, grdmax, Cu_min, Cu_max

      real lonmin, lonmax, latmin, latmax
      common /communicators_lonlat/
     &     lonmin, lonmax, latmin, latmax

      real*8 Cu_Adv3d,  Cu_W, Cu_Nbq_X, Cu_Nbq_Y, Cu_Nbq_Z
      integer i_cx_max, j_cx_max, k_cx_max
      common /diag_vars/ Cu_Adv3d,  Cu_W,
     &        i_cx_max, j_cx_max, k_cx_max
      real*8 volume, avgke, avgpe, avgkp, bc_crss


      common /communicators_rq/
     &          volume, avgke, avgpe, avgkp, bc_crss

!
!  The following common block contains process counters and model
! timers. These are used to measure CPU time consumed by different
! parallel threads during the whole run, as well as in various
! parallel regions, if so is needed. These variables are used purely
! for diagnostic/performance measurements purposes and do not affect
! the model results.
!
      real*4 CPU_time(0:31,0:NPP)
      integer proc(0:31,0:NPP),trd_count
      common /timers_roms/CPU_time,proc,trd_count

!
!  related variables
! === ====== =========
!
      logical EAST_INTER2, WEST_INTER2, NORTH_INTER2, SOUTH_INTER2
      logical EAST_INTER, WEST_INTER, NORTH_INTER, SOUTH_INTER
      logical CORNER_SW,CORNER_NW,CORNER_NE,CORNER_SE
      integer mynode, mynode2, ii,jj, p_W,p_E,p_S,p_N, p_SW,p_SE,
     & p_NW,p_NE,NNODES2
      common /comm_setup/ mynode, mynode2, ii,jj, p_W,p_E,p_S,p_N,
     & p_SW,p_SE, p_NW,p_NE, EAST_INTER, WEST_INTER, NORTH_INTER,
     & SOUTH_INTER, EAST_INTER2, WEST_INTER2, NORTH_INTER2, SOUTH_INTER2,
     & CORNER_SW,CORNER_NW,CORNER_NE,CORNER_SE,NNODES2


!
! Physical constants:
! ======== ==========

      real pi, deg2rad, rad2deg
      parameter (pi=3.14159265358979323846, deg2rad=pi/180.,
     &                                      rad2deg=180./pi)
!
! Earth radius [m]; Earth rotation [rad/s]; Acceleration of gravity [m/s^2];
! duration of the day in seconds and its inverse; Julian offset day.

      real Eradius, Erotation, g, day2sec,sec2day, jul_off,
     &     year2day,day2year
      parameter (Eradius=6371315.0,  Erotation=7.292115090e-5,
     &           day2sec=86400., sec2day=1./86400.,
     &           year2day=365.25, day2year=1./365.25,
     &           jul_off=2440000.)
!
! Acceleration of gravity (nondimensional for Soliton problem)
!
      parameter (g=9.81)
!
!  Specific heat [Joules/kg/degC] for seawater, it is approximately
!  4000, and varies only slightly (see Gill, 1982, Appendix 3).
!
      real Cp
      parameter (Cp=3985.0)

      real vonKar
      parameter (vonKar=0.41)
!
!   FillValue (Needed if the FILLVAL key is defined)
!   (See fillvalue.F subroutine)
      real spval
      parameter (spval=-999.0)
      logical mask_val
      parameter (mask_val = .true.)
!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!
!$AGRIF_DO_NOT_TREAT
      INTEGER :: ocean_grid_comm
      common /cpl_comm/ ocean_grid_comm
!$AGRIF_END_DO_NOT_TREAT


      include 'mpif.h'
      logical, parameter :: use_host_device = .TRUE.
      integer sub_X,size_X, sub_E,size_E   
      parameter (sub_X=Lm,  size_X=3*(sub_X+2*3)-1,
     &           sub_E=Mm,  size_E=3*(sub_E+2*3)-1)

      real ibuf_sndN(0:size_X), ibuf_revN(0:size_X),
     &     ibuf_sndS(0:size_X), ibuf_revS(0:size_X),
     &     jbuf_sndW(0:size_E), jbuf_sndE(0:size_E),
     &     jbuf_revW(0:size_E), jbuf_revE(0:size_E)

      real buf_snd4(3*3),     buf_snd2(3*3),
     &     buf_rev4(3*3),     buf_rev2(3*3),
     &     buf_snd1(3*3),     buf_snd3(3*3), 
     &     buf_rev1(3*3),     buf_rev3(3*3)

      common/buf_mpi/buf_snd4,buf_snd2,buf_rev4,buf_rev2,
     &               buf_snd1,buf_snd3,buf_rev1,buf_rev3,
     &     ibuf_sndN, ibuf_revN,
     &     ibuf_sndS, ibuf_revS,
     &     jbuf_sndW, jbuf_sndE,
     &     jbuf_revW, jbuf_revE


      integer size_Z,sub_X_3D,size_X_3D, sub_E_3D,size_E_3D

      parameter (size_Z=3*3*(N+1),
     &       sub_X_3D=(Lm+NSUB_X-1)/NSUB_X,
     &     size_X_3D=(N+1)*3*(sub_X_3D+2*3),
     &     sub_E_3D=(Mm+NSUB_E-1)/NSUB_E,
     &     size_E_3D=(N+1)*3*(sub_E_3D+2*3))

      real ibuf_sndN_3D(size_X_3D), ibuf_revN_3D(size_X_3D),
     &     ibuf_sndS_3D(size_X_3D), ibuf_revS_3D(size_X_3D),
     &     jbuf_sndW_3D(size_E_3D), jbuf_sndE_3D(size_E_3D),
     &     jbuf_revW_3D(size_E_3D), jbuf_revE_3D(size_E_3D)
      real buf_snd4_3D(size_Z), buf_snd2_3D(size_Z),
     &     buf_rev4_3D(size_Z), buf_rev2_3D(size_Z),
     &     buf_snd1_3D(size_Z), buf_snd3_3D(size_Z), 
     &     buf_rev1_3D(size_Z), buf_rev3_3D(size_Z)

      common/buf_mpi_3D/buf_snd4_3D, buf_snd2_3D,
     &                  buf_rev4_3D, buf_rev2_3D,
     &                  buf_snd1_3D, buf_snd3_3D,
     &                  buf_rev1_3D, buf_rev3_3D,
     &     ibuf_sndN_3D, ibuf_revN_3D,
     &     ibuf_sndS_3D, ibuf_revS_3D,
     &     jbuf_sndW_3D, jbuf_sndE_3D,
     &     jbuf_revW_3D, jbuf_revE_3D
!
! Nb of boundary points involved in communication
!
      integer Npts,ipts,jpts
      parameter (Npts=3)
      integer nmax
      real A(-1:Lm+2+padd_X,-1:Mm+2+padd_E,nmax)
CSDISTRIBUTE_RESHAPE A(BLOCK_PATTERN) BLOCK_CLAUSE
      integer Istr,Iend,Jstr,Jend, i,j,k, isize,jsize,ksize,
     &        req(8), status(MPI_STATUS_SIZE,8), ierr
      integer iter, mdii, mdjj
!
!======================================================================
! CROCO is a branch of ROMS developped at IRD, INRIA, 
! Ifremer, CNRS and Univ. Toulouse III  in France
! The two other branches from UCLA (Shchepetkin et al)
! and Rutgers University (Arango et al) are under MIT/X style license.
! CROCO specific routines (nesting) are under CeCILL-C license.
!
! CROCO website : http://www.croco-ocean.org
!======================================================================
!
      integer imin,imax,ishft, jmin,jmax,jshft
      if (ii.eq.0 .and. Istr.eq.1) then   ! Extra point on either
        imin=Istr-1                       ! side to accomodate ghost
      else                                ! points associated with
        imin=Istr                         ! PHYSICAL boundaries.
      endif
      if (ii.eq.NP_XI-1 .and. Iend.eq.Lmmpi) then
        imax=Iend+1
      else
        imax=Iend
      endif
      ishft=imax-imin+1

      if (jj.eq.0 .and. Jstr.eq.1) then   ! Extra point on either
        jmin=Jstr-1                       ! side to accomodate ghost
      else                                ! points associated with
        jmin=Jstr                         ! PHYSICAL boundaries.
      endif
      if (jj.eq.NP_ETA-1 .and. Jend.eq.Mmmpi) then
        jmax=Jend+1
      else
        jmax=Jend
      endif
      jshft=jmax-jmin+1


      ksize=Npts*Npts*nmax               ! message sizes for
      isize=Npts*ishft*nmax              ! corner messages and sides
      jsize=Npts*jshft*nmax              ! in XI and ETA directions
!
! Prepare to receive and send: sides....
!
                            !  Message passing split into two stages
                            !  in order to optimize Send-Recv pairing
                            !  in such a way that if one subdomain
      do iter=0,1           !  sends message to, say, its WESTERN
        mdii=mod(ii+iter,2) !  neighbor, that neighbor is preparing
        mdjj=mod(jj+iter,2) !  to receive this message first (i.e.
                            !  message coming from its EASTERN side),
                            !  rather than send his WEST
                            !  bound message, similarly to the first
                            !  subdomain.

!
! Prepare to receive and send: sides....
        if (mdii.eq.0) then
          if ((WEST_INTER2)) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do j=jmin,jmax
                do ipts=1,Npts
                  jbuf_sndW_3D(k+nmax*(j-jmin+(ipts-1)*jshft))=
     &                  A(ipts,j,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(jbuf_revW_3D,jbuf_sndW_3D)
            call MPI_Irecv (jbuf_revW_3D, jsize, MPI_DOUBLE_PRECISION,
     &                         p_W, 2, MPI_COMM_WORLD, req(1), ierr)
            call MPI_Send  (jbuf_sndW_3D, jsize, MPI_DOUBLE_PRECISION,
     &                         p_W, 1, MPI_COMM_WORLD,         ierr)
!$acc end host_data
          endif
        else
          if (EAST_INTER2) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do j=jmin,jmax
                do ipts=1,Npts
                  jbuf_sndE_3D(k+nmax*(j-jmin+(ipts-1)*jshft))=
     &                                       A(Lmmpi-Npts+ipts,j,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(jbuf_revE_3D,jbuf_sndE_3D)
            call MPI_Irecv (jbuf_revE_3D, jsize, MPI_DOUBLE_PRECISION,
     &                        p_E, 1, MPI_COMM_WORLD, req(2), ierr)
            call MPI_Send  (jbuf_sndE_3D, jsize, MPI_DOUBLE_PRECISION,
     &                        p_E, 2, MPI_COMM_WORLD,         ierr)
!$acc end host_data
          endif
        endif

        if (mdjj.eq.0) then
          if (SOUTH_INTER2) then
!$acc parallel loop if(compute_on_device) 
!$acc& default(present) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do i=imin,imax
                  ibuf_sndS_3D(k+nmax*(i-imin+(jpts-1)*ishft))=
     &                          A(i,jpts,k)
                enddo
              enddo
            enddo
!$acc update host(ibuf_sndS_3D(1:isize)) if(.not.use_host_device)
!$acc host_data if(compute_on_device.and.use_host_device)
!$acc&   use_device(ibuf_revS_3D,ibuf_sndS_3D)
            call MPI_Irecv (ibuf_revS_3D, isize, MPI_DOUBLE_PRECISION,
     &                         p_S, 4, MPI_COMM_WORLD, req(3), ierr)
            call MPI_Send  (ibuf_sndS_3D, isize, MPI_DOUBLE_PRECISION,
     &                         p_S, 3, MPI_COMM_WORLD,         ierr)
!$acc end host_data
          endif
        else
          if (NORTH_INTER2) then
!$acc parallel loop if(compute_on_device)
!$acc& default(present) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do i=imin,imax
                  ibuf_sndN_3D(k+nmax*(i-imin+(jpts-1)*ishft))=
     &                                         A(i,Mmmpi-Npts+jpts,k)
                enddo
              enddo
            enddo
!$acc update host(ibuf_sndN_3D(1:isize)) if(.not.use_host_device)
!$acc host_data if(compute_on_device.and.use_host_device)
!$acc&   use_device(ibuf_revN_3D,ibuf_sndN_3D)
            call MPI_Irecv (ibuf_revN_3D, isize, MPI_DOUBLE_PRECISION,
     &                         p_N, 3, MPI_COMM_WORLD, req(4), ierr)
            call MPI_Send  (ibuf_sndN_3D, isize, MPI_DOUBLE_PRECISION,
     &                         p_N, 4, MPI_COMM_WORLD,         ierr)
!$acc end host_data
          endif
        endif
!
! ...corners:
!
        if (mdii.eq.0) then
          if (CORNER_SW) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do ipts=1,Npts
                  buf_snd1_3D(k+nmax*(ipts-1+Npts*(jpts-1)))=
     &                 A(ipts,jpts,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(buf_rev1_3D,buf_snd1_3D)
            call MPI_Irecv (buf_rev1_3D, ksize, MPI_DOUBLE_PRECISION, p_SW,
     &                                6, MPI_COMM_WORLD, req(5),ierr)
            call MPI_Send  (buf_snd1_3D, ksize, MPI_DOUBLE_PRECISION, p_SW,
     &                                5, MPI_COMM_WORLD,        ierr)
!$acc end host_data
          endif
        else
          if (CORNER_NE) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do ipts=1,Npts
                  buf_snd2_3D(k+nmax*(ipts-1+Npts*(jpts-1)))=
     &                            A(Lmmpi+ipts-Npts,Mmmpi+jpts-Npts,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(buf_rev2_3D,buf_snd2_3D)
            call MPI_Irecv (buf_rev2_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_NE, 5, MPI_COMM_WORLD, req(6),ierr)
            call MPI_Send  (buf_snd2_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_NE, 6, MPI_COMM_WORLD,        ierr)
!$acc end host_data
          endif
        endif

        if (mdii.eq.1) then
          if (CORNER_SE) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do ipts=1,Npts
                  buf_snd3_3D(k+nmax*(ipts-1+Npts*(jpts-1)))=
     &                                A(Lmmpi+ipts-Npts,jpts,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(buf_rev3_3D,buf_snd3_3D)
            call MPI_Irecv (buf_rev3_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_SE, 8, MPI_COMM_WORLD, req(7),ierr)
            call MPI_Send  (buf_snd3_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_SE, 7, MPI_COMM_WORLD,        ierr)
!$acc end host_data
          endif
        else
          if (CORNER_NW) then
!$acc parallel loop if(compute_on_device) collapse(3)
            do k=1,nmax
              do jpts=1,Npts
                do ipts=1,Npts
                  buf_snd4_3D(k+nmax*(ipts-1+Npts*(jpts-1)))=
     &                                A(ipts,Mmmpi+jpts-Npts,k)
                enddo
              enddo
            enddo
!$acc host_data if(compute_on_device)
!$acc&   use_device(buf_rev4_3D,buf_snd4_3D)
            call MPI_Irecv (buf_rev4_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_NW, 7, MPI_COMM_WORLD, req(8),ierr)
            call MPI_Send  (buf_snd4_3D, ksize, MPI_DOUBLE_PRECISION, 
     &                  p_NW, 8, MPI_COMM_WORLD,        ierr)
!$acc end host_data
          endif
        endif
      enddo   !<-- iter

!
! Wait for completion of receive and fill ghost points: sides...
!
      if (WEST_INTER2) then
        call MPI_Wait (req(1),status(1,1),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do j=jmin,jmax
            do ipts=1,Npts
             A(ipts-Npts,j,k)=
     &          jbuf_revW_3D(k+nmax*(j-jmin+(ipts-1)*jshft))
            enddo
          enddo
        enddo
      endif
      if (WEST_INTER .and. .not. WEST_INTER2) then
        do k=1,nmax
          do j=jmin,jmax
            do ipts=1,Npts
              A(ipts-Npts,j,k)=A(ipts,j,k)
            enddo
          enddo
        enddo
      endif

      if (EAST_INTER2) then
        call MPI_Wait (req(2),status(1,2),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do j=jmin,jmax
            do ipts=1,Npts
              A(Lmmpi+ipts,j,k)=
     &                         jbuf_revE_3D(k+nmax*(j-jmin+(ipts-1)
     &                         *jshft))
            enddo
          enddo
        enddo
      endif
      if (EAST_INTER .and. .not. EAST_INTER2) then
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do j=jmin,jmax
            do ipts=1,Npts
              A(Lmmpi+ipts,j,k)=A(Lmmpi+ipts-Npts,j,k)
            enddo
          enddo
        enddo
      endif


      if (SOUTH_INTER2) then
        call MPI_Wait (req(3),status(1,3),ierr)
!$acc update device(ibuf_revS_3D(1:isize)) if(.not.use_host_device)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do jpts=1,Npts
            do i=imin,imax
              A(i,jpts-Npts,k)=ibuf_revS_3D(k+nmax*(i-imin+(jpts-1)
     &                        *ishft))
            enddo
          enddo
        enddo
      endif
      if (SOUTH_INTER .and. .not. SOUTH_INTER2) then
!$acc parallel loop if(compute_on_device) collapse(3) 
       do k=1,nmax
          do i=imin,imax
            do jpts=1,Npts
              A(i,jpts-Npts,k)=A(i,jpts,k)
            enddo
          enddo
        enddo
      endif

      if (NORTH_INTER2) then
        call MPI_Wait (req(4),status(1,4),ierr)
!$acc update device(ibuf_revN_3D(1:isize)) if(.not.use_host_device)
!$acc parallel loop if(compute_on_device) collapse(3)
        do k=1,nmax
          do jpts=1,Npts
            do i=imin,imax
              A(i,Mmmpi+jpts,k)=
     &          ibuf_revN_3D(k+nmax*(i-imin+(jpts-1)*ishft))
            enddo
          enddo
        enddo
      endif
      if (NORTH_INTER .and. .not. NORTH_INTER2) then
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do i=imin,imax
            do jpts=1,Npts
              A(i,Mmmpi+jpts,k)=A(i,Mmmpi+jpts-Npts,k)
            enddo
          enddo
        enddo
      endif
!
! ...corners:
!
      if (CORNER_SW) then
        call MPI_Wait (req(5),status(1,5),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(ipts-Npts,jpts-Npts,k)=
     &                           buf_rev1_3D(k+nmax*(ipts-1+Npts*(jpts-1)))
            enddo
          enddo
        enddo
      endif
      if (.not. CORNER_SW  .and.
     &   SOUTH_INTER .and.  WEST_INTER ) then
!$acc parallel loop if(compute_on_device) collapse(3) 
       do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(ipts-Npts,jpts-Npts,k)=
     &                           A(ipts,jpts,k)
            enddo
          enddo
        enddo
       endif

      if (CORNER_NE) then
        call MPI_Wait (req(6),status(1,6),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(Lmmpi+ipts,Mmmpi+jpts,k)=
     &                           buf_rev2_3D(k+nmax*(ipts-1+Npts*(jpts-1)))
            enddo
          enddo
        enddo
      endif
      if (.not. CORNER_NE  .and.
     &   NORTH_INTER .and.  EAST_INTER ) then
!$acc parallel loop if(compute_on_device) collapse(3) 
       do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(Lmmpi+ipts,Mmmpi+jpts,k)=
     &              A(Lmmpi+ipts-Npts,Mmmpi+jpts-Npts,k)
            enddo
          enddo
        enddo
       endif


      if (CORNER_SE) then
        call MPI_Wait (req(7),status(1,7),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(Lmmpi+ipts,jpts-Npts,k)=
     &                           buf_rev3_3D(k+nmax*(ipts-1+Npts*(jpts-1)))
            enddo
          enddo
        enddo
      endif
      if (.not. CORNER_SE .and.
     &   SOUTH_INTER .and.  EAST_INTER ) then
!$acc parallel loop if(compute_on_device) collapse(3)
       do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
               A(Lmmpi+ipts,jpts-Npts,k)=
     &                           A(Lmmpi+ipts-Npts,jpts,k)
            enddo
          enddo
        enddo
       endif


      if (CORNER_NW) then
        call MPI_Wait (req(8),status(1,8),ierr)
!$acc parallel loop if(compute_on_device) collapse(3) 
        do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(ipts-Npts,Mmmpi+jpts,k)=
     &                           buf_rev4_3D(k+nmax*(ipts-1+Npts*(jpts-1)))
            enddo
          enddo
        enddo
      endif
      if (.not. CORNER_NW  .and.
     &   NORTH_INTER .and.  WEST_INTER ) then
!$acc parallel loop if(compute_on_device) collapse(3) 
       do k=1,nmax
          do jpts=1,Npts
            do ipts=1,Npts
              A(ipts-Npts,Mmmpi+jpts,k)=A(ipts,Mmmpi+jpts-Npts,k)
            enddo
          enddo
        enddo
       endif


      return
      end









